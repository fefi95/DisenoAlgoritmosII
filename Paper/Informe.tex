\documentclass{ci5652}
\usepackage{graphicx,amssymb,amsmath}
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{paralist}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{csvsimple}
\usepackage[none]{hyphenat}

%-------------------------- Macros and Definitions ----------------------------%

% Add all additional macros here, do NOT include any additional files.

% The environments theorem (Theorem), invar (Invariant), lemma (Lemma),
% cor (Corollary), obs (Observation), conj (Conjecture), and prop
% (Proposition) are already defined in the ci5652.cls file.

%------------------------------------------------------------------------------%
%                                                                              %
%                                  TÍTULO                                      %
%                                                                              %
%------------------------------------------------------------------------------%

\title{Metaheurísticas para el problema de Aprendizaje de Pesos en Características}

\author{Stefani Castellanos
        \and
        Erick Silva}

%------------------------------------------------------------------------------%
%                                                                              %
%                                CONTENIDO                                     %
%                                                                              %
%------------------------------------------------------------------------------%

\begin{document}
\thispagestyle{empty}
\maketitle

%------------------------------------------------------------------------------%
%                                 RESUMEN                                      %
%------------------------------------------------------------------------------%

\begin{abstract}
*Inserte una descripción breve del paper.*

Palabras claves: Aprendizaje de Pesos en Características, Metaheurísticas,
\textit{Machine Learning}.
\end{abstract}

%------------------------------------------------------------------------------%
%                                INTRODUCCIÓN                                  %
%------------------------------------------------------------------------------%

\section*{Introducción}
% NO ESTÁ COMPLETO
Debido a la gran cantidad de información manejada actualmente, ha surgido la
necesidad de reducir el tamaño de dichos datos, sin embargo, esto genera el
problema de escoger correctamente que información es relevante para el
clasificador.\cite{Cano_2003} \\

Reducir el tamaño de los datos es posible a través de los siguientes métodos:

\begin{itemize}
  \item Seleccionando características del conjunto de datos, lo que reduce el
  tamaño de las columnas.
  \item Eliminando las instancias del conjunto de datos que aporte poca
  información.
  \item Determinando la importancia de las características, ya que proporciona
  información que permite guiar al clasificador y así, reducir el tiempo de
  procesamiento.
\end{itemize}

Este trabajo se enfocará en esta última, denominado el problema de \textit{Aprendizaje
de Pesos en Características}. Se utilizará un algoritmo \textit{greedy} (ávido)
denomindado RELIEF, dos metaheurísticas de trayectoría y dos poblacionales,
para finalmente compararlas y determinar cual de ellos es más adecuado para el
problema.

%------------------------------------------------------------------------------%
%                          DESCRIPCIÓN DEL PROBLEMA                            %
%------------------------------------------------------------------------------%

\section{Descripción del problema}

Antes de describir el problema APC, es necesario comprender en qué consiste
un problema de clasificación; se dispone de un conjunto de posibles clases
($C$) y un conjunto de datos ($X$), en donde una instancia $x_i$ es un vector
previamente clasificado y de la forma:
$$x_i = \langle f_1, f_2, \dots, f_n, c\rangle $$ en donde:

\begin{itemize}
  \item $f_i$ : el valor de cada característica.
  \item $n$ : la cantidad de característcas.
  \item $c$ : la clase a la que pertenece dicha instancia, con $c \in C$.
\end{itemize}

Es común particionar $X$ en dos subconjuntos que representen el conjunto de
entrenamiento y el de prueba, $Xe$ y $Xp$ respectivamente, de manera que $X?e$
pueda ser utilizado para que el algoritmo aprenda los parámetros que le
permitan clasificar correctamente todas sus instancias y utilizar $X_p$ para
validar los resultados obtenidos.\\

El problema del APC consiste en optimizar el rendimiento de un clasificador, a
partir de la inclusión de pesos asociados a las características del conjunto de
datos. Estos pesos ponderan la relevancia de cada característica en y modifican
su valor al momento de calcular las distancias entre instancias, de tal forma 
que los clasificadores que se construyan a partir de estos pesos sean certeros 
y/o más rápidos. En este caso en particular, el clasificador considerado será el
\textit{K-Nearest Neighbor} (K-vecinos más cercanos) con $K=1$ (1-NN).\\

El algoritmo K-NN asume que todas las instancias corresponden a puntos en un
espacio $n$-dimensional ($\Re^n$), en donde $n$ es la cantidad de
características del conjunto de datos. \cite{Mitchell_1997}. El algoritmo es el
siguiente:

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$, Conjunto de prueba $X_p$, $K$}
 \KwOut{Conjunto $X_p$ clasificados según $C$}
  \ForEach{$x_i \in X_p$}{
   vecinos = [ ];\;
   \ForEach{$x_j \in X_e$}{
    d = distancia($x_i, x_j$);\;
    agregar($x_j, d, vecinos$);\;
   }
  k\_vecinos = seleccionar\_cercanos($k, vecinos$);\;
  clasificar($x_i, k\_vecinos$);\;
 }
 \KwRet{$X_p$ clasificado}
 \vspace*{0.1cm}
 \caption{K-Nearest Neighbor}
\end{algorithm}

El proceso de aprendizaje de este clasificador consiste en almacenar una tabla
con las instancias correspondientes al $X_e$ junto a la clase asociada a cada 
uno de ellos. Dado una instancia $x_i \in X_p$, se calcula su distancia a todas
las otras que pertenecen al conjunto de entrenamiento y se escogen las $k$ más
cercanas \cite{Herrera_2017}; usualmente se determina la proximidad entre dos
ejemplos utilizando la distancia Euclideana. Finalmente, $x_i$ se clasifica  
según la clase mayoritaria grupo y se retorna el conjunto de pruebas clasificado.

%-------------------------- FUNCIÓN OBJETIVO ----------------------------------%

\subsection{Función objetivo}

Al ser APC un problema de optimización, es necesario establecer cual es la 
función que se desea mejorar. En este caso, dadas las características del
problema, resulta evidente que obtener una "buena" solución está fuertemente
relacionado con la cantidad de instancias clasificadas correctamiente usando
1-NN. El objetivo es encontrar el mejor vector de pesos que permita máximizar la
tasa de aciertos del clasificador 1-NN. Más específicamente:

\begin{equation}
  Max\ tasa(\text{1-NN}(X, W)) = 100 \times \frac{aciertos(X)}{total(X)}
\end{equation}

sujeto a:
\[
w_i = [0, 1] \ \ 1 \leq i \leq n
\]

donde:
\begin{itemize}
  \item $W = \langle w_1, \dots, w_n\rangle$ es una solución al problema.
  \item 1-NN es el clasificador k-NN con $k=1$ vecinos, generado a partir del
  conjunto de datos inicial e incluyendo el peso de $W$ al momento de calcular 
  las distancias.
  \item $X$ es el conjunto de datos sobre el que se evalúa el clasificador.
  \item $aciertos$ es la cantidad de instancias de $X$ clasificadas 
  correctamente por 1-NN.
  \item $total$ es la cantidad total de instancias de $X$.
\end{itemize}

%---------------------- REPRESENTACIÓN DE LA SOLUCIÓN -------------------------%

\subsection{Representación de la solución}

La solución al problema de Aprendizaje de Pesos en Características viene dado 
por $W = \langle w_1, \dots, w_n\rangle$, un vector de números reales de tamaño
$n$ (cantidad de características) en el que el valor de cada $w_i$  define el
peso que pondera a la  característica $f_i$, es decir, representa que tan
importante para distinguir un ejemplo de otro.\\

Cada $w_i$ debe pertenecer al intervalo $[0,1]$; los valores cercanos a 1 indican
que la característica es más importante. En caso de que algún valor quede fuera
de este intervalo, se debe normalizar, seleccionando al máximo valor del vector
y dividiendo todos los valores entre dicho número. 

%--------------------------- CONJUNTO DE DATOS --------------------------------%

\subsection{Conjunto de datos}

Con la expansión del área de Inteligencia Artificial, se ha convertido en una
necesidad contar con bases de datos que proporcionen información que permita a
los investigadores, educadores y estudiantes del área realizar análisis sobre 
los algoritmos de \textit{Machine Learning}. Una de las librerias más populares
en la actualidad es UCI Machine Learning Repository, que mantiene una colección
de conjuntos de datos, teorías de dominio y generadores de datos disponibles 
para toda la comunidad.\\

Para efectuar el análisis de las metaherísticas a utilizar para resolver el
problema APC se utilizarán cuatro librerias disponibles en 
\href{http://archive.ics.uci.edu/ml/index.php}{UCI}:

\begin{description}
  \item [Iris:] este es probablemente el conjunto de datos mejor conocido y citado
  en la literatura de reconocimiento de patrones. Contiene tres clases de 50
  instancias cada una, en donde cada clase corresponde a un tipo de planta Iris.
  Cada instancia posee las siguientes características: largo del sépalo, ancho 
  del sépalo, largo del pétalo y ancho del pétalo en centímetros. Las clases a
  predecir son: "Iris-Setosa", "Iris-Versicolor" e "Iris-Virginica". 
  \cite{UCI_Iris}.
  
  \item [Sonar:] es una base de datos de detección de materiales mediante señales 
  de sónar que "rebotan" en los objetos desde diferentes ángulos y bajo 
  condiciones varias, discriminando entre cilindros metálicos(\textit{mines}) y
  rocas(\textit{rocks}). Cuenta con 111 \textit{mines} y 97 \textit{rocks} donde
  cada instancia es un conjunto de 60 números entre 0.0 y 1.0 que representan la
  energía entre una banda en particular, integrada sobre un periodo dispuestas 
  en orden cresciente según el ángulo. Las clases a predecir son: "R"
  (\textit{rocks}) y "M" (\textit{mines}). \cite{UCI_Sonar}.
   
  \item [Winsconsin Diagnostic Breast Cancer:] es una base de datos contiene
  atributos calculados a partir de una imagen digitalizada de una aspiración con
  aguja fina de una masa en la mama. Se describen las características de los
  núcleos de las células presentes en la imagen. La tarea consiste en determinar
  si un tumor encontrado es benigno o maligno. Cuenta con 357 tumores beningnos
  y 212 malignos en donde cada instancia posee 30 caracteristicas, 10 valores
  reales computados por cada célula: radio, textura, perímetro, área, suavidad,
  compacto, concavidad, puntos cóncavos, simetría y dimensión fractal.
  \cite{UCI_WDBC}.

  \item [Spam Base:] es una base de datos de detección de SPAM (correo basura)
  frente a correo electrónico seguro. Cuenta de 460 ejemplos y 57 atributos en
  donde los primeros 48 corresponden al porcentaje de frecuencia de una palabra
  en particular en un correo, 6 corresponden al porcentaje de frecuencia de
  símbolos de puntuación y las últimas 3 son el promedio, máximo y la suma de la
  cantidad de letras en mayúsculas. Las clases a predecir son: 1 (spam), 0 
  (non-spam) que representan \cite{UCI_SpamBase}.
  
\end{description}

%------------------------------------------------------------------------------%
%                             ALGORITMO RELIEF                                 %
%------------------------------------------------------------------------------%

\section{Algortimo RELIEF}

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$}
 \KwOut{Vector de pesos $W$}
  W = {0, 0 \dots, 0}\;
  \ForEach{$x_i \in X_e$}{
   a = amigo\_mas\_cercano($x_i$);\;
   e = enemigo\_mas\_cercano($x_i$);\;
   \;
   /* Actualizar pesos de $W$ */\;
   \ForEach{$w_i \in W$}{
    dif\_amigo = diferencia($w_i, a$);\;
    dif\_enemigo = diferencia($w_i, e$);\;
    $w_i$ = $w_i$ - dif\_amigo + dif\_enemigo;\;
   }
  }
  normalizar($W$);\;
 \KwRet{$W$}
 \vspace*{0.1cm}
 \caption{RELIEF}
\end{algorithm}

%------------------------------------------------------------------------------%
%                              METAHEURÍSTICAS                                 %
%------------------------------------------------------------------------------%

\section{Metaheurísticas}
\subsection{Metaheurísticas de trayectoria}
\subsubsection{Enfriamiento Simulado (Simulated annealing)}
\subsubsection{Búsqueda Local Iterada (Iterated Local Search)}
\subsection{Metaheurísticas poblacional}
% POSIBLES METEHEURÍSTICAS
\subsubsection{Algoritmo Genético}
\subsubsection{Algoritmo Memético}

%------------------------------------------------------------------------------%
%                              IMPLEMENTACIÓN                                  %
%------------------------------------------------------------------------------%

\section{Implementación}
*Inserte Implementación*

%------------------------------------------------------------------------------%
%                                RESULTADOS                                    %
%------------------------------------------------------------------------------%

\section{Resultados}
*Inserte Resultados*\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/iris/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/sonar/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/spambase/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/wdbc/relief.csv}{}{\csvlinetotablerow}%

%------------------------------------------------------------------------------%
%                               CONCLUSIONES                                   %
%------------------------------------------------------------------------------%

\section*{Conclusiones}

Aquí concluyen.

%------------------------------ Bibliography ---------------------------------%

% Please add the contents of the .bbl file that you generate,  or add bibitem entries manually if you like.
% The entries should be in alphabetical order
\small
\bibliographystyle{abbrv}

\begin{thebibliography}{99}

\bibitem{Cano_2003}
Cano, J. Herrera, F. Lozano. M
\newblock Using evolutionary algorithms as Instance Selection for data
reduction in KDD: an experimental study.
\newblock {\em IEEE Transaction on Evolutionary computation}, 2003.

\bibitem{UCI_Iris}
Fisher, R.A. (1988). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/iris/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Herrera_2017}
Herrera, F.
\newblock Metaheurísticas. 
\newblock {\em Seminario 2: Problemas de optimización con técnicas
basadas en búsqueda local}, 2017. Disponible en: 
\url{http://sci2s.ugr.es/sites/default/files/files/Teaching/GraduatesCourses/Metaheuristicas/Sem02-Problemas-BusquedaLocal-MHs-16-17.pdf}

\bibitem{UCI_SpamBase}
Hopkins, M. (1999). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Mitchell_1997}
Mitchell, T.
\newblock Machine Learning.
\newblock {\em From Book News, Inc}, 1997.

\bibitem{UCI_Sonar}
Sejnowski, T (año). UCI Machine Learning Repository 
\newblock [\url{http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Talbi_2009}
Talbi E.
\newblock Metaheuristics: From desing to implementation
\newblock {\em University of Lille}, 2009.

\bibitem{UCI_WDBC}
Wolberg, W. (1995). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

%----TODO: ELIMINAR BIBITEM ----%
%\bibitem{so2005}
%EJEMPLO C. So and H. So.
%\newblock A groundbreaking result.
%\newblock {\em Journal of Everything}, 59(2):23--37, 2005.
%
\end{thebibliography}


\newpage
\section*{Apéndice}

Bla.

%------------------------ EL EJEMPLO DE LA PROFE -----------------------------%
%
%\section{La sección de ejemplo de la profe}
%Citan así~\cite{so2005}. ELIMINAR AL ESCRIBIR EL INFORME
%
%\begin{algorithm}
% \DontPrintSemicolon
% \vspace*{0.1cm}
% \KwIn{Descripcion}
% \KwOut{Descripcion}
% Primer paso\;
% Segundo\;
% \ForEach{$i = 1\dots n$}{
%  \If{Alguna condición}{
%   Algo aqui\;
%   }
% }
% \KwRet{Valor}
% \vspace*{0.1cm}
% \caption{Nombre}
%\end{algorithm}
%
\end{document}