\documentclass{ci5652}
\usepackage{graphicx,amssymb,amsmath}
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{paralist}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{csvsimple}
\usepackage[none]{hyphenat}

%-------------------------- Macros and Definitions ----------------------------%

% Add all additional macros here, do NOT include any additional files.

% The environments theorem (Theorem), invar (Invariant), lemma (Lemma),
% cor (Corollary), obs (Observation), conj (Conjecture), and prop
% (Proposition) are already defined in the ci5652.cls file.

%------------------------------------------------------------------------------%
%                                                                              %
%                                  TÍTULO                                      %
%                                                                              %
%------------------------------------------------------------------------------%

\title{Metaheurísticas para el problema de Aprendizaje de Pesos en Características}

\author{Stefani Castellanos
        \and
        Erick Silva}

%------------------------------------------------------------------------------%
%                                                                              %
%                                CONTENIDO                                     %
%                                                                              %
%------------------------------------------------------------------------------%

\begin{document}
\thispagestyle{empty}
\maketitle

%------------------------------------------------------------------------------%
%                                 RESUMEN                                      %
%------------------------------------------------------------------------------%

\begin{abstract}
*Inserte una descripción breve del paper.*

Palabras claves: Aprendizaje de Pesos en Características, Metaheurísticas,
\textit{Machine Learning}.
\end{abstract}

%------------------------------------------------------------------------------%
%                                INTRODUCCIÓN                                  %
%------------------------------------------------------------------------------%

\section*{Introducción}
% NO ESTÁ COMPLETO
Debido a la gran cantidad de información manejada actualmente, ha surgido la
necesidad de reducir el tamaño de dichos datos, sin embargo, esto genera el
problema de escoger correctamente que información es relevante para el
clasificador.\cite{Cano_2003} \\

Reducir el tamaño de los datos es posible a través de los siguientes métodos:

\begin{itemize}
  \item Seleccionando características del conjunto de datos, lo que reduce el
  tamaño de las columnas.
  \item Eliminando las instancias del conjunto de datos que aporte poca
  información.
  \item Determinando la importancia de las características, ya que proporciona
  información que permite guiar al clasificador y así, reducir el tiempo de
  procesamiento.
\end{itemize}

Este trabajo se enfocará en esta última, denominado el problema de \textit{Aprendizaje
de Pesos en Características}. Se utilizará un algoritmo \textit{greedy} (ávido)
denomindado RELIEF, dos metaheurísticas de trayectoría y dos poblacionales,
para finalmente compararlas y determinar cual de ellos es más adecuado para el
problema.

%------------------------------------------------------------------------------%
%                          DESCRIPCIÓN DEL PROBLEMA                            %
%------------------------------------------------------------------------------%

\section{Descripción del problema}

Antes de describir el problema APC, es necesario comprender en qué consiste
un problema de clasificación; se dispone de un conjunto de posibles clases
($C$) y un conjunto de datos ($X$), en donde una instancia $x_i$ es un vector
previamente clasificado y de la forma:
$$x_i = \langle f_1, f_2, \dots, f_n, c\rangle $$ en donde:

\begin{itemize}
  \item $f_i$ : el valor de cada característica.
  \item $n$ : la cantidad de característcas.
  \item $c$ : la clase a la que pertenece dicha instancia, con $c \in C$.
  \item $|X| = m$: cantidad de instancias.
  \item $|C| = p$ : cantidad de clases.
\end{itemize}

Es común particionar $X$ en dos subconjuntos que representen el conjunto de
entrenamiento y el de prueba, $Xe$ y $Xp$ respectivamente, de manera que $X?e$
pueda ser utilizado para que el algoritmo aprenda los parámetros que le
permitan clasificar correctamente todas sus instancias y utilizar $X_p$ para
validar los resultados obtenidos.\\

El problema del APC consiste en optimizar el rendimiento de un clasificador, a
partir de la inclusión de pesos asociados a las características del conjunto de
datos. Estos pesos ponderan la relevancia de cada característica en y modifican
su valor al momento de calcular las distancias entre instancias, de tal forma 
que los clasificadores que se construyan a partir de estos pesos sean certeros 
y/o más rápidos. En este caso en particular, el clasificador considerado será el
\textit{K-Nearest Neighbor} (K-vecinos más cercanos) con $K=1$ (1-NN).\\

El algoritmo K-NN asume que todas las instancias corresponden a puntos en un
espacio $n$-dimensional ($\Re^n$), en donde $n$ es la cantidad de
características del conjunto de datos. \cite{Mitchell_1997}. El algoritmo es el
siguiente:

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$, Conjunto de prueba $X_p$, $K$}
 \KwOut{Conjunto $X_p$ clasificados según $C$}
  \ForEach{$x_i \in X_p$}{
   vecinos = [ ];\;
   \ForEach{$x_j \in X_e$}{
    d = distancia($x_i, x_j$);\;
    agregar($x_j, d, vecinos$);\;
   }
  k\_vecinos = seleccionar\_cercanos($k, vecinos$);\;
  clasificar($x_i, k\_vecinos$);\;
 }
 \KwRet{$X_p$ clasificado}
 \vspace*{0.1cm}
 \caption{K-Nearest Neighbor}
\end{algorithm}

El proceso de aprendizaje de este clasificador consiste en almacenar una tabla
con las instancias correspondientes al $X_e$ junto a la clase asociada a cada 
uno de ellos. Dado una instancia $x_i \in X_p$, se calcula su distancia a todas
las otras que pertenecen al conjunto de entrenamiento y se escogen las $k$ más
cercanas \cite{Herrera_2017}; usualmente se determina la proximidad entre dos
ejemplos utilizando la distancia Euclideana. Finalmente, $x_i$ se clasifica  
según la clase mayoritaria grupo y se retorna el conjunto de pruebas clasificado.

%-------------------------- FUNCIÓN OBJETIVO ----------------------------------%

\subsection{Función objetivo}

Al ser APC un problema de optimización, es necesario establecer cual es la 
función que se desea mejorar. En este caso, dadas las características del
problema, resulta evidente que obtener una "buena" solución está fuertemente
relacionado con la cantidad de instancias clasificadas correctamiente usando
1-NN. El objetivo es encontrar el mejor vector de pesos que permita máximizar la
tasa de aciertos del clasificador 1-NN. Más específicamente:

\begin{equation}
  Max\ tasa(\text{1-NN}(X, W)) = 100 \times \frac{aciertos(X)}{total(X)}
\end{equation}

sujeto a:
\[
w_i = [0, 1] \ \ 1 \leq i \leq n
\]

donde:
\begin{itemize}
  \item $W = \langle w_1, \dots, w_n\rangle$ es una solución al problema.
  \item 1-NN es el clasificador k-NN con $k=1$ vecinos, generado a partir del
  conjunto de datos inicial. La distancia entre dos instancias se calculará 
  con la distancia euclideana pesada: 
  $$\sqrt{\sum_{i=1}^n (w_i * (x'_i - x''_i))^{2}}$$
  \item $X$ es el conjunto de datos sobre el que se evalúa el clasificador.
  \item $aciertos$ es la cantidad de instancias de $X$ clasificadas 
  correctamente por 1-NN.
  \item $total$ es la cantidad total de instancias de $X$.
\end{itemize}

%---------------------- REPRESENTACIÓN DE LA SOLUCIÓN -------------------------%

\subsection{Representación de la solución}

La solución al problema de Aprendizaje de Pesos en Características viene dado 
por $W = \langle w_1, \dots, w_n\rangle$, un vector de números reales de tamaño
$n$ (cantidad de características) en el que el valor de cada $w_i$  define el
peso que pondera a la  característica $f_i$, es decir, representa que tan
importante para distinguir un ejemplo de otro.\\

Cada $w_i$ debe pertenecer al intervalo $[0,1]$; los valores cercanos a 1 indican
que la característica es más importante. En caso de que algún valor quede fuera
de este intervalo, se debe normalizar, seleccionando al máximo valor del vector
y dividiendo todos los valores entre dicho número. 

%------------------------------------------------------------------------------%
%                             ALGORITMO RELIEF                                 %
%------------------------------------------------------------------------------%
\section{Algoritmo Relief}

Es un algoritmo para escoger características inspirado en el aprendizaje basado 
en instancias, detecta aquellos ejemplos que son estadísticamente relevantes 
para el concepto objetivo en tiempo lineal ($\Theta(nmp)$). Utiliza un 
\textit{threshold}, $\tau$ entre $0 \leq \tau \leq 1$ , que codifica la 
relevancia de una característica en particular \cite{Kira_1992};  en esta 
versión será omitido este parámetro puesto que las metaheurísticas utilizan el 
vector de pesos y no el vector de relevancias que se obtiene con $\tau$.\\

Relief utiliza la distancia euclideana $n$-dimensional para determinar el "amigo
más cercano" y el "enemigo más cercano" de una instancia $x_i$. Se denomina a 
una instancia "amigo más cercano" o "near-hit" a la instancia que pertenezca a 
la misma clase de $x_i$ y se encuentre a menor distancia. Se denomina a una 
instancia "enemigo más cercano" o "near-miss" a la instancia que pertenezca a 
una clase diferente a $x_i$ y se encuentre a menor distancia \cite{Kira_1992}.

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$}
 \KwOut{Vector de pesos $W$}
  W = {0, 0 \dots, 0}\;
  \ForEach{$x_i \in X_e$}{
   a = amigo\_mas\_cercano($x_i$);\;
   e = enemigo\_mas\_cercano($x_i$);\;
   \;
   /* Actualizar pesos de $W$ */\;
   \For{$i \dots n$}{
    dif\_amigo = diferencia($x_i, a$);\;
    dif\_enemigo = diferencia($x_i, e$);\;
    $w_i$ = $w_i$ - dif\_amigo + dif\_enemigo;\;
   }
  }
  normalizar($W$);\;
 \KwRet{$W$}
 \vspace*{0.1cm}
 \caption{RELIEF}
\end{algorithm}

La diferencia entre el valor $x_i$ y el amigo/enemigo más cercano está definido 
por: 
- Para los atributos con valores numéricos:
$$\text{diferencia}(a,b) = {(a - b)}^{2}$$
- Para los atributos con valores nominales:
\[
\text{diferencia}(a,b) = 
  \begin{cases}
    0 & \text{son el iguales}\\
    1 & \text{son diferentes}
  \end{cases} 
\]
%------------------------------------------------------------------------------%
%                              METAHEURÍSTICAS                                 %
%------------------------------------------------------------------------------%

\section{Metaheurísticas}

Las soluciones a muchos problemas de optimización son intratables, es decir, obtener la mejor respuesta podría tomar un tiempo potencialmente infinito, no ser resoluble o no se dispone de la capacidad computacional suficiente para resolverlo. Las metaheurísticas constituyen un conjunto de estrategías para guiar heurísticas a encontrar soluciones aceptables en un tiempo razonable para resolver un problema difícil o del que no se dispone información completa \cite{Talbi_2009}. Usualmente poseen un componente estocástico, por lo que la solución depende de las variables aleatorias generadas y se describen los resultados basados en resultados empíricos. Existen tres tipos de metaheurísticas: de trayectoria, poblacionales e hibridas.\\

En general, las meteheurísticas mejoran soluciones obtenidas anteriormentes. Sus componentes principales son:

\begin{itemize}
  \item Inicialización. Se requiere alguna solución inicial según la representación del problema particular, esta puede ser generada de manera aleatoria o utilizando algun algoritmo \textit{greedy}.
  \item Operador de vecindad. Se debe disponer de un algortimo que, a partir de otra solución, genere un conjunto de soluciones "vecinas". Este operador puede entenderse como una pequeña perturbación en alguna componente de la representación utilizada.
  \item Criterio de selección. Entre las soluciones que pertenecen a la vecindad, se debe escoger la "mejor". Existe diversas políticas, las más conocidas son: el mejor de toda la vecindad, el primer mejor y el mejor de una porción de la vecindad. La escogencia de algun criterio sobre otro depende del problema.
  \item Criterio de convergencia. Al mejorar una solución a partir de la anterior, es necesario contar con algún mecanismo que permita detener la ejecución de la metaheurística y devolver alguna solución. Los más conocidos son: detenerse al encontrar el óptimo (si este es conocido), luego de un número fijo de iteraciones y luego de un número fijo de iteraciones sin cambiar la mejor solución.
\end{itemize}

\subsection{Metaheurísticas de trayectoria}

Al resolver un problema de optimización, las metaheurísticas de trayectoria, utilizan una solución y realizan mejoras sobre esta, iterativamente; pueden ser entendidas como "caminatas" sobre el espacio de búsqueda del problema. Probablemente la metaherística de trayectoria más conocida es \textit{Local Search}, esta toma una solución inicial, encuentra sus vecinos y escoge el mejor según un criterio \cite{Talbi_2009}. 

\subsubsection{Búsqueda Local Iterada (Iterated Local Search)}

\textit{Local Search} podría quedar atrapado en un óptimo local y jamás llegar al global porque su intención es intensificar la búsqueda, por esta razón ILS supone una mejora sobre LS. Es una de las metaheurísticas más fáciles de implementar,
primero se aplica LS a la solución inicial; luego, en cada iteración, se realiza una perturbación del óptimo local y se repite el proceso hasta un cumplir el criterio de aceptación \cite{Talbi_2009}.


\subsubsection{Enfriamiento Simulado (Simulated annealing)}

\subsection{Metaheurísticas poblacional}
% POSIBLES METEHEURÍSTICAS
\subsubsection{Algoritmo Genético}
\subsubsection{Algoritmo Memético}

%------------------------------------------------------------------------------%
%                              IMPLEMENTACIÓN                                  %
%------------------------------------------------------------------------------%

\section{Implementación}
*Inserte Implementación*

%--------------------------- CONJUNTO DE DATOS --------------------------------%

\subsection{Conjunto de datos}

Con la expansión del área de Inteligencia Artificial, se ha convertido en una
necesidad contar con bases de datos que proporcionen información que permita a
los investigadores, educadores y estudiantes del área realizar análisis sobre 
los algoritmos de \textit{Machine Learning}. Una de las librerias más populares
en la actualidad es UCI Machine Learning Repository, que mantiene una colección
de conjuntos de datos, teorías de dominio y generadores de datos disponibles 
para toda la comunidad.\\

Para efectuar el análisis de las metaherísticas a utilizar para resolver el
problema APC se utilizarán cuatro librerias disponibles en 
\href{http://archive.ics.uci.edu/ml/index.php}{UCI}:

\begin{description}
  \item [Iris:] este es probablemente el conjunto de datos mejor conocido y citado
  en la literatura de reconocimiento de patrones. Contiene tres clases de 50
  instancias cada una, en donde cada clase corresponde a un tipo de planta Iris.
  Cada instancia posee las siguientes características: largo del sépalo, ancho 
  del sépalo, largo del pétalo y ancho del pétalo en centímetros. Las clases a
  predecir son: "Iris-Setosa", "Iris-Versicolor" e "Iris-Virginica". 
  \cite{UCI_Iris}.
  
  \item [Sonar:] es una base de datos de detección de materiales mediante señales 
  de sónar que "rebotan" en los objetos desde diferentes ángulos y bajo 
  condiciones varias, discriminando entre cilindros metálicos(\textit{mines}) y
  rocas(\textit{rocks}). Cuenta con 111 \textit{mines} y 97 \textit{rocks} donde
  cada instancia es un conjunto de 60 números entre 0.0 y 1.0 que representan la
  energía entre una banda en particular, integrada sobre un periodo dispuestas 
  en orden cresciente según el ángulo. Las clases a predecir son: "R"
  (\textit{rocks}) y "M" (\textit{mines}). \cite{UCI_Sonar}.
   
  \item [Winsconsin Diagnostic Breast Cancer:] es una base de datos contiene
  atributos calculados a partir de una imagen digitalizada de una aspiración con
  aguja fina de una masa en la mama. Se describen las características de los
  núcleos de las células presentes en la imagen. La tarea consiste en determinar
  si un tumor encontrado es benigno o maligno. Cuenta con 357 tumores beningnos
  y 212 malignos en donde cada instancia posee 30 caracteristicas, 10 valores
  reales computados por cada célula: radio, textura, perímetro, área, suavidad,
  compacto, concavidad, puntos cóncavos, simetría y dimensión fractal.
  \cite{UCI_WDBC}.

  \item [Spam Base:] es una base de datos de detección de SPAM (correo basura)
  frente a correo electrónico seguro. Cuenta de 460 ejemplos y 57 atributos en
  donde los primeros 48 corresponden al porcentaje de frecuencia de una palabra
  en particular en un correo, 6 corresponden al porcentaje de frecuencia de
  símbolos de puntuación y las últimas 3 son el promedio, máximo y la suma de la
  cantidad de letras en mayúsculas. Las clases a predecir son: 1 (spam), 0 
  (non-spam) que representan \cite{UCI_SpamBase}.
  
\end{description}


%------------------------------------------------------------------------------%
%                                RESULTADOS                                    %
%------------------------------------------------------------------------------%

\section{Resultados}
*Inserte Resultados*\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/iris/no_weights.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/sonar/no_weights.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/spambase/no_weights.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/wdbc/no_weights.csv}{}{\csvlinetotablerow}%
\\
RELIEF\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/iris/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/sonar/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/spambase/relief.csv}{}{\csvlinetotablerow}%
\\
\csvreader[
  respect all, 
  autotabular
  ]{statistics/wdbc/relief.csv}{}{\csvlinetotablerow}%

%------------------------------------------------------------------------------%
%                               CONCLUSIONES                                   %
%------------------------------------------------------------------------------%

\section*{Conclusiones}

Aquí concluyen.

%------------------------------ Bibliography ---------------------------------%

% Please add the contents of the .bbl file that you generate,  or add bibitem entries manually if you like.
% The entries should be in alphabetical order
\small
\bibliographystyle{abbrv}

\begin{thebibliography}{99}

\bibitem{Cano_2003}
Cano, J. Herrera, F. Lozano. M
\newblock Using evolutionary algorithms as Instance Selection for data
reduction in KDD: an experimental study.
\newblock {\em IEEE Transaction on Evolutionary computation}, 2003.

\bibitem{UCI_Iris}
Fisher, R.A. (1988). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/iris/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Herrera_2017}
Herrera, F.
\newblock Metaheurísticas. 
\newblock {\em Seminario 2: Problemas de optimización con técnicas
basadas en búsqueda local}, 2017. Disponible en: 
\url{http://sci2s.ugr.es/sites/default/files/files/Teaching/GraduatesCourses/Metaheuristicas/Sem02-Problemas-BusquedaLocal-MHs-16-17.pdf}

\bibitem{UCI_SpamBase}
Hopkins, M. (1999). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Kira_1992}
Kira, K., Rendell, A.
\newblock A practical approach to feature selection, 1992.

\bibitem{Mitchell_1997}
Mitchell, T.
\newblock Machine Learning.
\newblock {\em From Book News, Inc}, 1997.

\bibitem{UCI_Sonar}
Sejnowski, T (año). UCI Machine Learning Repository 
\newblock [\url{http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Talbi_2009}
Talbi E.
\newblock Metaheuristics: From desing to implementation
\newblock {\em University of Lille}, 2009.

\bibitem{UCI_WDBC}
Wolberg, W. (1995). UCI Machine Learning Repository 
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

%----TODO: ELIMINAR BIBITEM ----%
%\bibitem{so2005}
%EJEMPLO C. So and H. So.
%\newblock A groundbreaking result.
%\newblock {\em Journal of Everything}, 59(2):23--37, 2005.
%
\end{thebibliography}


\newpage
\section*{Apéndice}

Bla.

%------------------------ EL EJEMPLO DE LA PROFE -----------------------------%
%
%\section{La sección de ejemplo de la profe}
%Citan así~\cite{so2005}. ELIMINAR AL ESCRIBIR EL INFORME
%
%\begin{algorithm}
% \DontPrintSemicolon
% \vspace*{0.1cm}
% \KwIn{Descripcion}
% \KwOut{Descripcion}
% Primer paso\;
% Segundo\;
% \ForEach{$i = 1\dots n$}{
%  \If{Alguna condición}{
%   Algo aqui\;
%   }
% }
% \KwRet{Valor}
% \vspace*{0.1cm}
% \caption{Nombre}
%\end{algorithm}
%
\end{document}