\documentclass{ci5652}
\usepackage{graphicx,amssymb,amsmath}
\usepackage[utf8]{inputenc}
% \usepackage[spanish]{babel}
\usepackage{hyperref}
\usepackage{subfigure}
\usepackage{paralist}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{csvsimple}
\usepackage[none]{hyphenat}

\graphicspath{{graphics/}}
%-------------------------- Macros and Definitions ----------------------------%

% Add all additional macros here, do NOT include any additional files.

% The environments theorem (Theorem), invar (Invariant), lemma (Lemma),
% cor (Corollary), obs (Observation), conj (Conjecture), and prop
% (Proposition) are already defined in the ci5652.cls file.

%------------------------------------------------------------------------------%
%                                                                              %
%                                  TÍTULO                                      %
%                                                                              %
%------------------------------------------------------------------------------%

\title{Metaheurísticas para el problema de Aprendizaje de Pesos en Características}

\author{Stefani Castellanos
        \and
        Erick Silva}

%------------------------------------------------------------------------------%
%                                                                              %
%                                CONTENIDO                                     %
%                                                                              %
%------------------------------------------------------------------------------%

\begin{document}
\thispagestyle{empty}
\maketitle

%------------------------------------------------------------------------------%
%                                 RESUMEN                                      %
%------------------------------------------------------------------------------%

\begin{abstract}

{\small Dado un conjunto de datos, se desea conocer que atributo es más
importante que otro al momento de distingüir la clase a la que pertenece una
instancia de otra. Esta es la problemática que plantea Aprendizaje de Pesos en
Características. Se procederá a resolver utilizando cuatro metaheurísticas
diferentes inicializadas de manera aleatoria o con un algoritmo que aprovecha la
información de los datos para guiar la búsqueda en el espacio: Relief. Los
mejores resultados serán aquellos que tengan una mejor relación calidad-tiempo
para cada conjunto de datos considerado.\\

Palabras claves: Aprendizaje de Pesos en Características, Metaheurísticas,
\textit{Machine Learning}}.
\end{abstract}

%------------------------------------------------------------------------------%
%                                INTRODUCCIÓN                                  %
%------------------------------------------------------------------------------%

\section*{Introducción}

Una de las preocupaciones más grandes del hombre ha sido crear información,
analizarla y permitir que perdure en el tiempo. Al entrar en la tecnológica esta
sed de conocimiento se ha afianzado y ha crecido.\\

Actualmente, se maneja una gran cantidad de información, sin embargo, no toda es
relevante para una aplicación especifica. Por esta razón ha surgido la necesidad
de reducir el tamaño de los datos, pero ¿Cómo escoger la información de manera
adecuada?. En Inteligencia Artificial y Minería de Datos esta ha sido una
interrogante constante debido a que se desea evitar consumir demasiado tiempo
procesando la información sin pérdida de precisión.\\

Reducir el tamaño de los datos es posible a través de los siguientes métodos:

\begin{itemize}
  \item Seleccionando características del conjunto de datos, lo que reduce el
  tamaño de las columnas.
  \item Eliminando las instancias del conjunto de datos que aporte poca
  información.
  \item Determinando la importancia de las características, ya que proporciona
  información que permite guiar al clasificador y así, reducir el tiempo de
  procesamiento.\cite{Cano_2003}
\end{itemize}

Este trabajo se enfocará en esta última, este problema de optimización  lleva el
nombre de Aprendizaje de Pesos en Características. Usualmente, ponderar los
atributos para mejorar la tasa de aciertos de un clasificador hace uso de Redes
Neurales, sin embargo estas pueden consumir demasiado tiempo. Se utilizará un
algoritmo \textit{greedy} (ávido) denominado Relief, dos metaheurísticas de
trayectoría y dos poblacionales, para compararlas y determinar cuál de ellos es
más adecuado para el problema.\\

Es indispensable contar con varios conjuntos de datos que permitan realizar la
fase experimental y así concretar los resultados. En este caso, fueron
seleccionados cinco: Iris, Sonar, WDBC, Spambase y Waveform. El primero
constituye un conjunto pequeño, los siguientes dos son medianos y los últimos
son consideradas problemas con gran cantidad de datos.\\

Primeramente se realizará la descripción del problema en términos matemáticos y
se explicará en que consiste el clasificador utilizado. Luego, se detallará el
funcionamiento del algoritmo Relief y las metaheurísticas Iterated Local Search,
Simulated Annealing, Scatter Search y Differential Evolution. Se ofrecerán
resultados de acuerdos a los experimentos realizados para cada algoritmo con
cada conunto de datos. Finalmente, se concluirá según lo analizado en la sección
descrita anteriormente.

%------------------------------------------------------------------------------%
%                          DESCRIPCIÓN DEL PROBLEMA                            %
%------------------------------------------------------------------------------%

\section{Descripción del problema}

Antes de describir el problema APC, es necesario comprender en qué consiste
un problema de clasificación; se dispone de un conjunto de posibles clases
($C$) y un conjunto de datos ($X$), en donde una instancia $x_i$ es un vector
previamente clasificado y de la forma:
$$x_i = \langle f_1, f_2, \dots, f_n, c\rangle $$ en donde:

\begin{itemize}
  \item $f_i$ : el valor de cada característica.
  \item $n$ : la cantidad de característcas.
  \item $c$ : la clase a la que pertenece dicha instancia, con $c \in C$.
  \item $|X| = m$: cantidad de instancias.
  \item $|C| = p$ : cantidad de clases.
\end{itemize}

Es común particionar $X$ en dos subconjuntos que representen el conjunto de
entrenamiento y el de prueba, $X_e$ y $X_p$ respectivamente. $X_e$ es utilizado
para que el algoritmo aprenda los parámetros que le permitan clasificar
correctamente todas sus instancias y así, utilizar $X_p$ para validar los
resultados obtenidos.\\

El problema del Aprendizaje de Pesos en Características consiste en optimizar el
rendimiento de un clasificador, a partir de la inclusión de pesos asociados a
las características del conjunto de datos. Los pesos ponderan la relevancia de
cada característica y modifican su valor al momento de calcular las distancias
entre instancias, de tal forma que los clasificadores que se construyan a partir
de estos, sean más certeros y/o más rápidos. En este caso en particular, el
clasificador considerado será el \textit{K-Nearest Neighbor} (K-vecinos más
cercanos) con $K=1$ (1-NN).\\

El algoritmo K-NN asume que todas las instancias corresponden a puntos en un
espacio $n$-dimensional ($\Re^n$), en donde $n$ es la cantidad de
características del conjunto de datos. \cite{Mitchell_1997}. El algoritmo es el
siguiente:

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$, Conjunto de prueba $X_p$, $K$}
 \KwOut{Conjunto $X_p$ clasificados según $C$}
  \ForEach{$x_i \in X_p$}{
   vecinos = [ ];\;
   \ForEach{$x_j \in X_e$}{
    d = distancia($x_i, x_j$);\;
    agregar($x_j, d, vecinos$);\;
   }
  k\_vecinos = seleccionar\_cercanos($k, vecinos$);\;
  clasificar($x_i, k\_vecinos$);\;
 }
 \KwRet{$X_p$ clasificado}
 \vspace*{0.1cm}
 \caption{K-Nearest Neighbor}
\end{algorithm}

El proceso de aprendizaje de este clasificador consiste en almacenar una tabla
con las instancias correspondientes al $X_e$ junto a la clase asociada a cada
uno de ellos. Dado una instancia $x_i \in X_p$, se calcula su distancia a todas
las otras que pertenecen al conjunto de entrenamiento y se escogen las $k$ más
cercanas \cite{Herrera_2017}; usualmente se determina la proximidad entre dos
ejemplos utilizando la distancia Euclideana. Finalmente, $x_i$ se clasifica
según la clase mayoritaria grupo y se retorna el conjunto de pruebas clasificado.
%ORDEN DE K-NN

%-------------------------- FUNCIÓN OBJETIVO ----------------------------------%

\subsection{Función objetivo}

Al ser APC un problema de optimización, es necesario establecer cuál es la
función que se desea mejorar. En este caso, dadas las características del
problema, resulta evidente que obtener una "buena" solución está fuertemente
relacionado con la cantidad de instancias clasificadas correctamiente usando
1-NN. El objetivo es encontrar el mejor vector de pesos que permita máximizar la
tasa de aciertos del clasificador 1-NN. Más específicamente:

\begin{equation}
  \max\ tasa(\text{1-NN}(X, W)) = 100 \times \frac{aciertos(X)}{total(X)}
\end{equation}

sujeto a:
\[
w_i = [0, 1] \ \ 1 \leq i \leq n
\]

donde:
\begin{itemize}
  \item $W = \langle w_1, \dots, w_n\rangle$ es una solución al problema.
  \item 1-NN es el clasificador k-NN con $k=1$ vecinos, generado a partir del
  conjunto de datos inicial.
  \item $X$ es el conjunto de datos sobre el cual se evalúa el clasificador.
  \item $aciertos$ es la cantidad de instancias de $X$ clasificadas
  correctamente por 1-NN.
  \item $total$ es la cantidad total de instancias de $X$.
\end{itemize}

%---------------------- REPRESENTACIÓN DE LA SOLUCIÓN -------------------------%

\subsection{Representación de la solución}

La solución al problema de Aprendizaje de Pesos en Características viene dado
por $W = \langle w_1, \dots, w_n\rangle$, un vector de números reales de tamaño
$n$ (cantidad de características) en el que el valor de cada $w_i$  define el
peso que pondera a la  característica $f_i$, es decir, representa que tan
importante es para distinguir a que clase pertenece un ejemplo.\\

Cada $w_i$ debe pertenecer al intervalo $[0,1]$; los valores cercanos a 1 indican
que la característica es más importante. En caso de que algún valor quede fuera
de este intervalo, se debe normalizar, seleccionando el máximo valor del vector
y dividiendo todos los valores entre dicho número.

%------------------------------------------------------------------------------%
%                             ALGORITMO RELIEF                                 %
%------------------------------------------------------------------------------%
\section{Algoritmo Relief}

Las metaheurísticas suelen ser inicializadas con soluciones generadas
aleatoriamente, sin embargo, esto puede producir un retardo en la ejecución o
desembocar en una convergencia prematura en un óptimo local, por lo que se
requiere de un algoritmo eficiente que tome información del problema y guie las
búsqueda de manera inteligente. Para el problema considerado en este artículo,
existe tal heurística: Relief.\\

Relief, es un algoritmo de selección de características inspirado en el
aprendizaje basado en instancias, detecta aquellos ejemplos que son
estadísticamente relevantes para el concepto objetivo en tiempo lineal
($\Theta(nmp)$). Utiliza un \textit{threshold}, $\tau$ entre
$0 \leq \tau \leq 1$ , que codifica la relevancia de una característica en
particular \cite{Kira_1992};  en esta versión será omitido este parámetro puesto
que las metaheurísticas utilizan el vector de pesos y no el vector de
relevancias que se obtiene con $\tau$.\\

Relief utiliza la distancia euclideana $n$-dimensional para determinar el "amigo
más cercano" y el "enemigo más cercano" de una instancia $x_i$. Se denomina a
una instancia "amigo más cercano" o "near-hit" a la instancia que pertenezca a
la misma clase de $x_i$ y se encuentre a menor distancia. Se denomina a una
instancia "enemigo más cercano" o "near-miss" a la instancia que pertenezca a
una clase diferente a $x_i$ y se encuentre a menor distancia \cite{Kira_1992}.

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Conjunto de entrenamiento $X_e$}
 \KwOut{Vector de pesos $W$}
  W = {0, 0 \dots, 0}\;
  \ForEach{$x_i \in X_e$}{
   a = amigo\_mas\_cercano($x_i$);\;
   e = enemigo\_mas\_cercano($x_i$);\;
   \;
   /* Actualizar pesos de $W$ */\;
   \For{$i \dots n$}{
    dif\_amigo = diferencia($x_i, a$);\;
    dif\_enemigo = diferencia($x_i, e$);\;
    $w_i$ = $w_i$ - dif\_amigo + dif\_enemigo;\;
   }
  }
  normalizar($W$);\;
 \KwRet{$W$}
 \vspace*{0.1cm}
 \caption{Relief}
\end{algorithm}

La diferencia entre el valor $x_i$ y el amigo/enemigo más cercano está definido
por: \\
- Para los atributos con valores numéricos:
$$\text{diferencia}(a,b) = {(a - b)}^{2}$$
- Para los atributos con valores nominales:
\[
\text{diferencia}(a,b) =
  \begin{cases}
    0 & \text{son el iguales}\\
    1 & \text{son diferentes}
  \end{cases}
\]
%------------------------------------------------------------------------------%
%                              METAHEURÍSTICAS                                 %
%------------------------------------------------------------------------------%

\section{Metaheurísticas}

Las soluciones a muchos problemas de optimización son intratables, es decir,
obtener la mejor respuesta podría tomar un tiempo potencialmente infinito, no
ser resoluble o no se dispone de la capacidad computacional suficiente para
resolverlo. Las metaheurísticas constituyen un conjunto de estrategías para
guiar heurísticas en la tarea de encontrar soluciones aceptables en un tiempo
razonable para resolver un problema difícil o del que no se dispone información
completa \cite{Talbi_2009}. Usualmente poseen un componente estocástico, por lo
que la solución depende de las variables aleatorias generadas y se describen los
resultados basados en observaciones empíricas. Existen tres tipos de
metaheurísticas: de trayectoria, poblacionales e hibridas.\\

En general, las meteheurísticas mejoran soluciones obtenidas anteriormente. Sus
componentes principales son:

\begin{itemize}
  \item Inicialización. Se requiere alguna solución inicial según la
  representación del problema particular, esta puede ser generada de manera
  aleatoria o utilizando algún algoritmo \textit{greedy}.
  \item Operador de vecindad. Se debe disponer de un algortimo que, a partir de
  otra solución, genere un conjunto de soluciones "vecinas". Este operador puede
  entenderse como una pequeña perturbación en alguna componente de la
  representación utilizada.
  \item Criterio de selección. Entre las soluciones que pertenecen a la
  vecindad, se debe escoger la "mejor". Existen diversas políticas, las más
  conocidas son: el mejor de toda la vecindad, el primer mejor y el mejor de una
  porción de la vecindad. Escoger algún criterio sobre otro depende del
  problema.
  \item Criterio de convergencia. Al mejorar una solución a partir de la
  anterior, es indispensable contar con algún mecanismo que permita detener la
  ejecución de la metaheurística y devolver alguna solución. Los más conocidos
  son: detenerse al encontrar el óptimo (si este es conocido), luego de un
  número fijo de iteraciones y luego de un número fijo de iteraciones sin
  modificar la mejor solución.

\end{itemize}

%--------------------- Metaheurísticas de trayectoria -------------------------%

\subsection{Metaheurísticas de trayectoria}

Al resolver un problema de optimización, las metaheurísticas de trayectoria,
utilizan una solución y realizan mejoras sobre esta, iterativamente; pueden ser
entendidas como "caminatas" sobre el espacio de búsqueda del problema.
Probablemente la metaheurística de trayectoria más conocida es
Local Search, que toma una solución inicial, encuentra sus vecinos y
escoge el mejor según un criterio \cite{Talbi_2009}.

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwOut{Mejor solución s*}
  s* = gen\_sol();\;
  \While{!criterio\_convergencia()}{
    V = gen\_vecinos(s');\;
    s' = seleccionar\_mejor(V);\;
    \If{costo(s) $<$ costo(s)}{
      s = s';\;
    }
  }
 \KwRet{s}
 \vspace*{0.1cm}
 \caption{Local Search}
\end{algorithm}

%--------------------------- Iterated Local Search ----------------------------%

\subsubsection{Búsqueda Local Iterada (Iterated Local Search)}

Local Search podría quedar atrapado en un óptimo local y jamás llegar al global
ya que se función es intensificar la búsqueda en una vencindad. Para mitigar
este riegos se crea una de las metaheurísticas más fáciles de implementar:
Iterated Local search, que supone una mejora sobre LS. Primero se aplica LS a la
solución inicial; luego, en cada iteración, se realiza una perturbación del
óptimo local y se repite el proceso hasta un cumplir el criterio de aceptación
\cite{Talbi_2009}.\\

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwOut{Mejor solución s*}
  s = gen\_sol();\;
  s* = local\_search(s);\;
  \While{!criterio\_convergencia()}{
    s = perturbar(s*);\;
    s' = local\_search(s');\;
    \If{$s* < s'$}{
      s* = s';\;
    }
  }
 \KwRet{s*}
 \vspace*{0.1cm}
 \caption{Iterated Local Search}
\end{algorithm}


%--------------------------- Simulated Annealing ------------------------------%

\subsubsection{Recocido Simulado (Simulated Annealing)}

Su nombre proviene del proceso físico de recocer sólidos, en el que un sólido
cristalino es calentado y luego se deja enfriar muy lentamente hasta que alcance
su configuración más estable y estructuralmente superior, por lo tanto está
libre de defectos del cristal \cite{Glover_2003}. En términos de resolver un
problema de optimización, el sólido representa una solución, "calentarlo" es
perturbar la solución hasta que la temperatura se estabilice y el "enfriamiento"
es una función no-creciente.\\

En cada iteración, Simulated Annealing, genera dos posibles soluciones: la
actual y otra, las cuales son comparadas. Aquellas que mejoren la actual
son siempre aceptadas; también existe una función probabilística dependiente de
la temperatura que permite aceptar alguna solución considerada "inferior" y así,
escapar de un mínimo local. Típicamente es una función no-creciente en cada
iteración del algoritmo y al aproximarse la temperatura a cero la probabilidad
de aceptar una solución que no mejora es menor \cite{Glover_2003}. La
temperatura sólo decrece cuando se ha alcanzado una condición de equilibrio.\\

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{$T\_inicial$}
 \KwOut{Mejor solución s}
  s = gen\_sol();\;
  T = T\_inicial;\;
  \While{!criterio\_convergencia()}{
    \While{!equilibrio}{
      /* Temperatura inicial */;\;
      s' = gen\_sol();\;
      \If{costo($s$) $<$ costo($s'$)}{
        s = s';\;
      }
      \Else{
        r = rand();\;
        delta = costo($s$) - costo($s'$);\;
        acep = 1/$(1+\exp^{1 + delta / T});$\;
        \If{r $<$ acep}{
          s = s';\;
        }
      }
    }
    T = actualizar\_temp();\;
  }
 \KwRet{s}
 \vspace*{0.1cm}
 \caption{Simulated Annealing}
\end{algorithm}

%---------------------- Metaheurísticas poblacionales -------------------------%

\subsection{Metaheurísticas poblacionales}

La mayoría de estos algoritmos están inspirados en fenómenos naturales y el
comportamiento de los animales. Estas metaheurísticas toman una población
inicial que está constituida por un conjunto de posibles soluciones del
problema. Luego, iterativamente, se generan nuevos individuos (soluciones) a
través de "cruces" y mutaciones en los genes (componentes del vector), los
cuales son seleccionados para reemplazar a algunos de la actual población,
creando una nueva. Este proceso se repite hasta alcanzar un criterio de
convergencia dado \cite{Talbi_2009}.\\

Los cruces puede ser entendidos como una combinación de dos individuos de la
población para crea otros nuevos, preservando algunas características de los
originales. El operador de mutación efectúa una perturbación de un hijo
generado; el propósito de este es realizar variaciones en la población que
permitan explorar nuevos lugares en el espacio de solución, es decir,
diversificar la búsqueda.

%------------------------------- Scatter Search -------------------------------%

\subsubsection{Scatter Search}

Es un metaheurística evolutiva y poblacional que recombina soluciones
seleccionadas de un "conjunto de referencia" para construir otras nuevas. El
método comienza generando una población inicial cuyos individuos satisfacen un
criterio de diversidad y calidad. El conjunto de referencia es construido
seleccionando buenas representaciones de la población que se cruzan para proveer
otras iniciales y luego mejorarlas con procedimientos basados en metaheurísticas
de trayectoria como Local Search. De acuerdo con esto, el conjunto de referencia
es actualizado para que contenga soluciones de buena calidad y otras que
permitan diversificar la búsqueda. El proceso itera hasta que un criterio de
parada se satisface \cite{Talbi_2009}.\\

Scatter Search aprovecha la información proveída por una heurística para crear
las soluciones "buenas" e intensificar la búsqueda alrededor de estos espacios.

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Población $P$}
 \KwOut{Mejor solución p\_mejor}
  $P$ = mejorar\_pop($P$);\;
  refSet = inicializar\_buenos\_individuos($P$);\;
  \While{!criterio\_convergencia()}{
    \ForEach{$r \in refSet$}{
      \ForEach{$r' \in refSet$}{
        s = combinar(r, r');\;
        s* = mejorar(s);\;
        $P$ = actualizar(s*);\;
      }
    }
    $P$ = ordenar($P$);\;
    refSet = mejores($P$);\;
    p\_mejor = mejor($P$);\;
  }
 \KwRet{$p\_mejor$}
 \vspace*{0.1cm}
 \caption{Scatter Search}
\end{algorithm}

%--------------------------- Diferential Evolution ----------------------------%

\subsubsection{Differential Evolution}

Es un algoritmo evolutivo que genera su población inicial de manera aleatoria y
con un tamaño mayor o igual a 4. Suele utilizarse para problemas de optimización
contínuos pues cada individuo tiene componentes con números reales.\\

El operador de cruce que utiliza está basado en la combinación líneal de las
soluciones utilizando la distancia entre las mismas. Dado un padre $x$ y tres
otro individuos seleccionados aleatoriamente $r1, r2$ y $r3$, se crea un nuevo
vector $u = r1 + F(r2 - r3)$. $F$ representa un factor permite añadir un peso o
importancia a la diferencia entre $r2$ y es tal que $r3$ y $F \in (0, 1)$
\cite{Glover_2003}. Finalmente se reemplaza $p$, con $p'$, de la
siguiente manera:

\[
p'_i =
  \begin{cases}
    u_i & \text{si } rand < CR\\
    p_i & \text{si } rand \geq CR
  \end{cases}
\]

donde:
\begin{itemize}
  \item $CR \in (0,1)$ : \textit{Crossover Constant}, un parámetro que
  representa la probabilidad de cruce.
  \item $rand$ : el número aleatorio entre 0 y 1 para escoger un componente u
  otro.
  \item $1 \leq i \leq n $ : índice para cada característica.
\end{itemize}

\begin{algorithm}
 \DontPrintSemicolon
 \vspace*{0.1cm}
 \KwIn{Población $P$, $F, CR$}
 \KwOut{Mejor solución p\_mejor}
  \While{!criterio\_convergencia()}{
    \ForEach{$p \in P$}{
      \For{$i = 0 \dots n - 1$}{
        /* Mutar y cruzar */\;
        r = rand($0, 1$)\;
        \If{$r < CR$}{
          $u_i = r1_i + F(r2_i - r3_i)$;\;
        }
        \Else{
          $u_i = p_i$\;
        }
        /* Reemplazar según la regla */\;
        $p'_i$ = reemplazar($p_i, u_i$)
      }
    }
    p\_mejor = mejor($P$);\;
  }
 \KwRet{$p\_mejor$}
 \vspace*{0.1cm}
 \caption{Differential Evolution}
\end{algorithm}

%------------------------------------------------------------------------------%
%                              IMPLEMENTACIÓN                                  %
%------------------------------------------------------------------------------%

\section{Implementación}

Para desarrollar las metaheurísticas y algoritmos explicados anteriormente y
obtener los resultados se utilizó en lenguaje de programación C++ que se
caracteriza por ser altamente eficiente.\\

En cuanto a la implementación de 1-NN, se retorna el porcentaje de instancias
clasificadas correctamente para el conjunto de datos dado y el cálculo de la
cercanía entre dos instancias se realiza utilizando la fórmula de distancia
euclideana pesada con la importancia de cada caraterística:
$$\sqrt{\sum_{i=1}^n (w_i * (x_i - x'_i))^{2}}$$

Para modelar la solución al problema planteado se utilizó un vector de números
reales con doble precisión (\texttt{double}). El operador para generar vecinos
selecciona un número aleatorio, $posToChange$, que representa que posición del
vector  a modificar, luego se escoge otro número aleatorio $rand \in [-1, 1]$
y se procede de la siguiente manera:
$$w_{posToChange}' =  w_{posToChange} + rand$$
\[
  W' =
  \begin{cases}
    w_{posToChange}' = 0 & \text{si } w_{posToChange}' < 0\\
    normalizar(W')       & \text{si } w_{posToChange}' \geq 1\\
    W'                   & \text{de lo contrario}
  \end{cases}
\]

Con respecto a las metaheurísticas, el criterio para seleccionar el mejor vecino
es "el mejor de una porción" ya que el "mejor" de todos no era factible puesto
que los vecinos de una solución específica son infinitos. Se utilizaron dos
criterios de convergencia: "cantidad fija de iteraciones" y "cantidad de
iteraciones sin cambios". A continuación se específica el criterio de
convergencia de las metaherísticas implementadas:

\begin{itemize}
  \item Local Search : Cantidad fija de iteraciones. Si la mejor solución no
  varía después de dos iteraciones se considera que hubo convergencia.
  \item Iterated Local Search : Cantidad de iteraciones sin cambios.
  \item Simulated Annealing: Cantidad fija de iteraciones. Si la mejor solución
  no varía después de dos iteraciones se considera que hubo convergencia
  \item Scatter Search: ambas.
  \item Differential Evolution: ambas.
\end{itemize}

Como método de perturbación de una solución se utilizó la idea para generar un
vecino pero no se modifica una posición del vector sino dos; esto se realiza
para aquellas metaheurísticas en las que se requiere un operador de vecidad
más amplio como ILS.\\

Simulated Annealing es uno de los algoritmos implementados que posee más
sutilezas y variaciones; requiere que se planifique el enfriamiento
(\textit{Cooling Schedule}). Para alcanzar el estado de equilibrio (en donde no
se varía la temperatura) se realiza un número de transiciones estáticas en las
que se fijó una cantidad de iteraciones. Existen diversas funciones para
disminuir la temperatura, en este caso se utilizó una función logaritmica que
decrece según el número de iteraciones $i$: $$\frac{T_{inicial}}{\log(i)}$$

Las metaheurísticas poblacionales requieren de un mecanismo que permita combinar
soluciones para crear nuevas, esto se denomina operador de cruce. Debido a que
el vector de pesos está compuesto por números reales, se implementó el operador
Blend Alpha Crossover que selecciona dos padres, digamos $Y$ y $Z$, para generar
dos hijos a partir de estos ($C1$ y $C2$) y para cada una de sus componentes,
$i$ se genera un número aleatorio dentro del intervalo
$[\min(y_i, z_i) - \alpha*diff,\max(y_i, z_i) - \alpha*diff]$ en donde $\alpha$
es un parámetro entre $[0,1]$ y $diff = |y_i - z_i|$. En particular, se utiliza
en Scatter Search ya que Diferential Evolution posee su propio operador de
cruce.\\

Para obtener el subconjunto de entrenamiento y el de pruebas para un conjunto de
datos específico se procede a desordenar el arreglo de instancias utilizando un
generador de números aleatorios con una semilla, de manera que la partición se
mantenga en cada corrida del algoritmo. Se seleccionan las primeras para
entrenamiento y el resto para pruebas según un número real dado, por ejemplo:
0.7 genera 70\% para entrenar y 30\% para las pruebas.\\


%------------------------------------------------------------------------------%
%                                EXPERIMENTO                                   %
%------------------------------------------------------------------------------%

\section{Experimento}

%--------------------------- CONJUNTO DE DATOS --------------------------------%

\subsection{Conjunto de datos}

Con la expansión del área de Inteligencia Artificial, se ha convertido en una
necesidad contar con bases de datos que proporcionen información que permita a
los investigadores, educadores y estudiantes del área realizar análisis sobre
los algoritmos de \textit{Machine Learning}. Una de las librerias más populares
en la actualidad es UCI Machine Learning Repository, que mantiene una colección
de conjuntos de datos, teorías de dominio y generadores de datos disponibles
para toda la comunidad.\\

Para efectuar el análisis de las metaheurísticas a utilizar para resolver el
problema APC se utilizan cuatro librerias disponibles en
\href{http://archive.ics.uci.edu/ml/index.php}{UCI}:

\begin{description}
  \item [Iris:] este es probablemente el conjunto de datos mejor conocido y citado
  en la literatura de reconocimiento de patrones. Contiene tres clases de 50
  instancias cada una, en donde cada clase corresponde a un tipo de planta Iris.
  Cada instancia posee las siguientes características: largo del sépalo, ancho
  del sépalo, largo del pétalo y ancho del pétalo en centímetros. Las clases a
  predecir son: "Iris-Setosa", "Iris-Versicolor" e "Iris-Virginica".
  \cite{UCI_Iris}.

  \item [Sonar:] es una base de datos de detección de materiales mediante señales
  de sónar que "rebotan" en los objetos desde diferentes ángulos y bajo
  condiciones varias, discriminando entre cilindros metálicos(\textit{mines}) y
  rocas(\textit{rocks}). Cuenta con 111 \textit{mines} y 97 \textit{rocks} donde
  cada instancia es un conjunto de 60 números entre 0.0 y 1.0 que representan la
  energía entre una banda en particular, integrada sobre un periodo dispuestas
  en orden cresciente según el ángulo. Las clases a predecir son: "R"
  (\textit{rocks}) y "M" (\textit{mines}). \cite{UCI_Sonar}.

  \item [Winsconsin Diagnostic Breast Cancer:] es una base de datos contiene
  atributos calculados a partir de una imagen digitalizada de una aspiración con
  aguja fina de una masa en la mama. Se describen las características de los
  núcleos de las células presentes en la imagen. La tarea consiste en determinar
  si un tumor encontrado es benigno o maligno. Cuenta con 357 tumores beningnos
  y 212 malignos en donde cada instancia posee 30 caracteristicas, 10 valores
  reales computados por cada célula: radio, textura, perímetro, área, suavidad,
  compacto, concavidad, puntos cóncavos, simetría y dimensión fractal.
  \cite{UCI_WDBC}.

  \item [Spambase:] es una base de datos de detección de SPAM (correo basura)
  frente a correo electrónico seguro. Cuenta de 4601 ejemplos y 57 atributos en
  donde los primeros 48 corresponden al porcentaje de frecuencia de una palabra
  en particular en un correo, 6 corresponden al porcentaje de frecuencia de
  símbolos de puntuación y las últimas 3 son el promedio, máximo y la suma de la
  cantidad de letras en mayúsculas. Las clases a predecir son: 1 (spam), 0
  (non-spam) que representan \cite{UCI_SpamBase}.

  \item [Waveform:] contiene datos de ondas que fueron generadas con un programa
  en C. Dispone de 5000 instancias, 40 atributos con valores contínuos entre
  [0, 6], y 3 clases de ondas que represetan la característica a predecir.
  \cite{UCI_Waveform}.

\end{description}

%--------------------- PARTICIONAMIENTO DE LOS DATOS --------------------------%

\subsection{Particionamiento de los datos}

Los conjuntos de datos mencionados anteriormente fueron particionados en dos
subconjuntos: entrenamiento y prueba; se utilizó el 60\% y 40\% de los datos
respectivamente. Cuando se analiza el comportamiento de una metaheurística
probabilística, se desearía que el resultado obtenido no estuviera sesgado por
una secuencia aleatoria concreta que pueda influir positiva o negativamente en
las decisiones tomadas durante su ejecución. Para minimizar el impacto de esto,
se realizaron varias particiones utilizando diferentes semillas para desordenar
los datos.

%----------------------- DESCRIPCIÓN DEL EXPERIMENTO --------------------------%

\subsection{Descripción del experimento}

Se realizaron dos tipos de experimentos. El primero de ellos consistió en
utilizar diferentes parámetros para cada una de las metaheurísticas  y con sólo
dos particiones, con el objetivo de encontrar los parámetros más adecuados
(entre los probados) para cada uno de los conjuntos de datos. En las tablas 1-45
se encuentran los resultados del promedio de las corridas.\\

El segundo experimento se basa en los mejores parámetros obtenidos según los
resultados promedio del primero y se realizan 5 particiones en las que se
obtiene la media. En las tablas 46-100 se encuentran los resultados. En
concreto, se consideraron los siguientes vectores de pesos (solución):

\begin{itemize}
  \item Sin pesos: todas las características tienen el mismo peso (igual a 1).
  \item Relief: el resultado de ejecutar el algoritmo Relief.
  \item ILS aleatorio: el resultado de ejecutar Iterated Local Search tomando
  como solución inicial un vector aleatorio.
  \item ILS relief: el resultado de ejecutar Iterated Local Search tomando como
  solución inicial el resultado de relief.
  \item SA aleatorio: el resultado de ejecutar Simulated Annealing tomando como
  solución inicial un vector aleatorio.
  \item SA relief: el resultado de ejecutar Simulated Annealing tomando como
  solución inicial el resultado de relief.
  \item SS aleatorio: el resultado de ejecutar Scatter Search en donde todos los
  individuos de la población son aleatorios.
  \item SS relief: el resultado de ejecutar Scatter Search en donde uno de los
  individuos es el resultado de Relief y el resto de la población es aleatorios.
  \item DE: el resultado de ejecutar Diferential Evolution con una población
  aleatoria.
\end{itemize}

Los resultados de estos experimentos fueron obtenidos en una computadora
portátil de 64 bits con un procesador Intel Core 2 Duo de 2.00 ghz con 3GB de
memoria RAM.

%------------------------------------------------------------------------------%
%                                RESULTADOS                                    %
%------------------------------------------------------------------------------%

\section{Análisis de resultados}

Primeramente, se desea conocer que mejora supone el uso de Relief para guiar el
aprendizaje del clasificador 1-NN, por esta razón se comparan los resultados
obtenidos para cada conjunto de datos utilizando la heurística y "sin pesos", es
decir, cada componente de $W$ es igual a 1.\\

\includegraphics[width=\columnwidth]{no_weights-Relief_Iris}
{\small Figura 1. Conjunto de datos: Iris. Comparación entre 1-NN sin considerar
pesos y con la heurística Relief.}\\

\includegraphics[width=\columnwidth]{no_weights-Relief_Sonar}
{\small Figura 2. Conjunto de datos: Sonar. Comparación entre 1-NN sin
considerar pesos y con la heurística Relief.}\\

En el caso del conjunto de datos Iris y Sonar (Figura 1 y 2), el porcentaje de
aciertos obtenidos al excluir los pesos presentan una mejora poco significativa
con respecto a Relief. La diferencia entre ellos es cercano al 2\% en promedio.
\\

\includegraphics[width=\columnwidth]{no_weights-Relief_WDBC}
{\small Figura 3. Conjunto de datos: WDBC. Comparación entre 1-NN sin considerar
pesos y con la heurística Relief.}\\

Para Wisconsin Diagnostic Breast Cancer Relief incrementa el promedio del
porcentaje de aciertos en apróximadamente 6\% con respecto a "sin pesos"\\

\includegraphics[width=\columnwidth]{no_weights-Relief_Spambase}
{\small Figura 4. Conjunto de datos: Spambase. Comparación entre 1-NN sin
considerar pesos y con la heurística Relief.}\\

En cuanto a Spambase es evidente que el uso de Relief perjudica la obtención de aciertos, esto se debe probablemente a que contiene datos muy dispersos con varios atributo igual a 0, lo que impide que la heurística diferencie claramente un clase de otra.

\includegraphics[width=\columnwidth]{no_weights-Relief_Waveform}
{\small Figura 5. Conjunto de datos: Waveform. Comparación entre 1-NN sin considerar pesos y con la heurística Relief.}\\

Se puede obeservar que el promedio de aciertos mejora con el uso de Relief en un 4\%. En general, la metaheurística proporciona buenos resultados para los conjuntos de datos utilizados en tiempos aceptables; alrededor de 1 segundo para los conjuntos considerados pequeños (Iris, Sonar y WDBC) y 5 segundos para los grandes (Spambase y Waveform).\\

Con relación a las metaheurísticas, se realizaron ejecuciones de cada algoritmo con los párametros que se consideraron mejores según el primer experimento. Los resultados para Iris estos se obtuvieron con los siguientes parámetros:
\begin{itemize}
  \item LS: 50 iteraciones, 20 vecinos.
  \item ILS: 50 iteraciones, 20 vecinos, 3 iteraciones sin cambios.
  \item SA: 2 iteraciones, 8 vecinos, 100 de temperatura inicial y 100 iteraciones de estabilización.
  \item SS: 10 iteraciones, 10 tamaño de la población, 6 iteraciones sin cambios
  \item DE: 2 iteraciones, 5 tamaño de la poblacion, 0.5 CR, 0.7 de Factor de Escalamiento.
\end{itemize}

\includegraphics[width=\columnwidth]{metaheuristicas_Iris}
{\small Figura 6. Conjunto de datos: Iris. Comparación entre todas las metaheurísticas.}\\

Se puede observar en la Figura 6 que cualquier meteheurística permite clasificar con alta precisión, alredero de 97\%, los diferentes tipos de flores que componen a este conjunto de datos. Con respecto a las metaheurísticas de trayectoria, en ILS aleatorio se observa un resultado atípico puesto que una de las particiones consideradas permitió obtener un 100\% de aciertos. Por su parte, SA presenta resultados (99\% en promedio) similares, sin importar como se inicialice la solución que se optimizará.\\

Las metaheurísticas de trayectoria no varían de manera importante con respecto a las anteriores, sin embargo, DE converge más rápidamente por lo que su precesión es cerca de 2\% peor a las demás. En cuanto al tiempo, ninguna sobrepasa los 0.5 segundos. La mejor de todas es Simulated Annealing con Relief ya que su tiempo es menor al de Scatter Search y obtienen los mismos reultados en promedio.\\

Para el conjunto de datos Sonar se utilizaron los siguientes parámetros:
\begin{itemize}
  \item LS: 250 iteraciones, 150 vecinos.
  \item ILS: 250 iteraciones, 150 vecinos, 3 iteraciones sin cambios.
  \item SA: 250 iteraciones, 4 vecinos, 50 de temperatura inicial y 100 iteraciones de estabilización.
  \item SS: 250 iteraciones, 10 tamaño de la población, 8 iteraciones sin cambios
  \item DE: 150 iteraciones, 10 tamaño de la poblacion, 0.5 CR, 0.5 de Factor de Escalamiento.
\end{itemize}

\includegraphics[width=\columnwidth]{metaheuristicas_Sonar}
{\small Figura 7. Conjunto de datos: Sonar. Comparación entre todas las metaheurísticas.}\\

Sobre las metaheurísticas de trayectoria, SA es indiscutiblemente mejor a ILS y puesto que supera el promedio por apróximadamente 4\% en prácticamente el mismo tiempo de ejecución. También es mejor que LS por 7\% a pesar de que el tiempo que toma es mayor: 3 segundos de diferencia.\\

Es difícil definir claramente cuál de las dos metaheurísticas poblacionales presentan mejores resultados debido a que SS supera a DE en un 3\% pero su tiempo de ejecución es el triple. La mejora de Simulated Annealing con respecto a Scatter Search es del 3\%, sin embargo tarda 3 segundos más, por lo que se considera mejor SS.

Para el conjunto de datos Winsconsin Diagnostic Breast Cancer se utilizaron los siguientes parámetros:
\begin{itemize}
  \item LS: 50 iteraciones, 30 vecinos.
  \item ILS: 50 iteraciones, 30 vecinos, 3 iteraciones sin cambios.
  \item SA: 40 iteraciones, 4 vecinos, 100 de temperatura inicial y 100 iteraciones de estabilización.
  \item SS: 40 iteraciones, 10 tamaño de la población, 8 iteraciones sin cambios
  \item DE: 50 iteraciones, 10 tamaño de la poblacion, 0.7 CR, 0.3 de Factor de Escalamiento.
\end{itemize}

\includegraphics[width=\columnwidth]{metaheuristicas_WDBC}
{\small Figura 8. Conjunto de datos: WDBC. Comparación entre todas las metaheurísticas.}\\

La metehaurística de trayectoria que determina con mayor certeza si una mujer tiene o no cáncer de mama según el conjunto de datos es SA, ya que muestra una tasa de aciertos de apróximadamente 94\% y los resultados se obtuvieron en un tiempo relativamente corto, de 5 segundos apóximadamente. ILS presenta un porcentaje mayo de errores, además consume más tiempo.

En este caso SS tarda 4 veces más que DE y en promedio obtiene un 2\% más de aciertos, por lo que DE es más apropiado para este problema; sin embargo Simulated Annealing con Relief es claramente superior, tanto en calidad como en tiempo.

Spambase fue la instancia más complicada de clasificar y que consumió más tiempo por lo que en el experimento 1 se realizaron menos pruebas; se escogieron los siguientes parámetros para cada metaheurística:

\begin{itemize}
  \item LS: 20 iteraciones, 15 vecinos.
  \item ILS: 10 iteraciones, 5 vecinos, 3 iteraciones sin cambios.
  \item SA: 10 iteraciones, 4 vecinos, 100 de temperatura inicial y 100 iteraciones de estabilización.
  \item SS: 2 iteraciones, 5 tamaño de la población, 6 iteraciones sin cambios.
  \item DE: 15 iteraciones, 5 tamaño de la poblacion, 0.7 CR, 0.3 de Factor de Escalamiento.
\end{itemize}

\includegraphics[width=\columnwidth]{metaheuristicas_Spambase}
{\small Figura 9. Conjunto de datos: Spambase. Comparación entre todas las metaheurísticas.}\\

En est problema particular, ninguna de las metaheurísticas inicializadas con Relief presentaron un incremento considerable en la cantidad de aciertos, por el contrario, disminuyeron. En cuanto a las metaheurísticas de trayectoria, LS fue la que obtuvo mejores resultados, 87\% en promedio, en la mitad del tiempo que SA.

DE supera el promedio de SS en un 4\% y de LS en 2\% en apróximadamente el mismo tiempo de cómputo que ambos, por lo que es el mejor para clasificar si un correo determinado es considerado "basura".

Los resultados obtenidos para Waveform utilizaron los siguientes parámetros:
\begin{itemize}
  \item LS: 15 iteraciones, 10 vecinos.
  \item ILS: 2 iteraciones, 3 vecinos, 4 iteraciones sin cambios.
  \item SA: 2 iteraciones, 2 vecinos, 100 de temperatura inicial y 100 iteraciones de estabilización.
  \item SS: 2 iteraciones, 5 tamaño de la población, 2 iteraciones sin cambios.
  \item DE: 2 iteraciones, 5 tamaño de la poblacion, 0.5 CR, 0.7 de Factor de Escalamiento.
\end{itemize}

\includegraphics[width=\columnwidth]{metaheuristicas_Waveform}
{\small Figura 10. Conjunto de datos: Waveform. Comparación entre todas las metaheurísticas.}\\

Es evidente el algoritmo Relief mejora los resultados de las todas las metheurísticas de manera considerable, en promedio 6\% apróximadamente por el mismo tiempo de ejecución. A pesar de que ILS obtiene resultados un poco mejores que SA el tiempo que consume llegando a esta tasa de aciertos es más del triple que SA. Por otro lado, DE obtiene resultados con un 2\% más de errores que SS, sin embargo tarda casi 6 veces menos. Simulated Annealing con Relief es la metaheurística como mejor relación calidad-tiempo para el para clasificar los tres tipos de olas.

Finalmente, es imperativo resaltar que ninguna de las metaheurísticas propuestas incrementa la tasa de aciertos de manera significativa con respecto al algoritmo Relief, a excepción de Spambase.

%------------------------------------------------------------------------------%
%                               CONCLUSIONES                                   %
%------------------------------------------------------------------------------%

\section*{Conclusiones}

Se exploraron cuatro metaheurísticas para resolver el problema de Aprendizaje de Pesos en Características, típicamente resuelto utilizando redes neurales. En este caso se utilizó el clasificador 1-NN y algún algoritmo que permita ponderar los atributos de un conjunto de datos dado.\\

Las metaheurísticas consideradas incluyeron dos de trayectoria y dos poblacionales: Iterated Local Search, Simulated Annealing, Scatter Search y Differential Evolution. Adicionalmente se realizaron inicializaciones diferentes, primero se utilizó una solución generada aleatoriamnte y luego una creada con el algoritmo Relief.\\

En términos generales, la metaheurística que presentó mejores resultados según su tiempo y calidad de la solución fue Simulated Annealing. Sin embargo, al analizar más profudamente los resultados, se puede notar que no constituye una mjora considerable con respecto a Relief.\\

Dadas las características de los conjuntos de datos estudiados, se puede concluir que el uso de alguna de las metaheurísticas propuestas está justificado cuando los datos son dispersos, puesto que Relief no genera una solución los suficientemente confiable.\\

Por otro lado, es necesario realizar más experimentos para lograr obtener una caracterización de los problemas. Una de los posibles experimentos a realizar es perturbar más la solución en aquellos algoritmos que así lo requieran, como ILS. Otro posible experimento sería modificar los valores del $\alpha$ en Blend Alpha Crossover y analizar como varían los resultados.\\

Por último, se propone llevar a cabo estos experimentos en un computador con mejores recursos, ya que los resultados podrían mejorar considerablemente.


%------------------------------------------------------------------------------%
%                               BIBLIOGRAFÍA                                   %
%------------------------------------------------------------------------------%
\newpage
\small
\bibliographystyle{abbrv}

\begin{thebibliography}{99}

\bibitem{UCI_Waveform}
Breiman, L. (1988). UCI Machine Learning Repository
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/waveform/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Cano_2003}
Cano, J. Herrera, F. Lozano. M
\newblock Using evolutionary algorithms as Instance Selection for data
reduction in KDD: an experimental study.
\newblock {\em IEEE Transaction on Evolutionary computation}, 2003.

\bibitem{UCI_Iris}
Fisher, R.A. (1988). UCI Machine Learning Repository
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/iris/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Glover_2003}
Glover, F
\newblock Handbook of metaheuristics.
\newblock {\em Operation resarch and management science}, 2003.

\bibitem{Herrera_2017}
Herrera, F.
\newblock Metaheurísticas.
\newblock {\em Seminario 2: Problemas de optimización con técnicas
basadas en búsqueda local}, 2017. Disponible en:
\url{http://sci2s.ugr.es/sites/default/files/files/Teaching/GraduatesCourses/Metaheuristicas/Sem02-Problemas-BusquedaLocal-MHs-16-17.pdf}

\bibitem{UCI_SpamBase}
Hopkins, M. (1999). UCI Machine Learning Repository
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Kira_1992}
Kira, K., Rendell, A.
\newblock A practical approach to feature selection, 1992.

\bibitem{Mitchell_1997}
Mitchell, T.
\newblock Machine Learning.
\newblock {\em From Book News, Inc}, 1997.

\bibitem{UCI_Sonar}
Sejnowski, T (año). UCI Machine Learning Repository
\newblock [\url{http://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

\bibitem{Talbi_2009}
Talbi E.
\newblock Metaheuristics: From desing to implementation
\newblock {\em University of Lille}, 2009.

\bibitem{UCI_WDBC}
Wolberg, W. (1995). UCI Machine Learning Repository
\newblock [\url{https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/}].
\newblock Irvine, CA: University of California, School of Information and Computer Science.

%----TODO: ELIMINAR BIBITEM ----%
%\bibitem{so2005}
%EJEMPLO C. So and H. So.
%\newblock A groundbreaking result.
%\newblock {\em Journal of Everything}, 59(2):23--37, 2005.
%
\end{thebibliography}

%------------------------------------------------------------------------------%
%                                APÉNDICE                                      %
%------------------------------------------------------------------------------%

\newpage
\small
\section*{Apéndice}

\subsection*{Experimento 1}
\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 1. Conjunto de Datos: Iris, Algoritmo: LS aleatorio.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 2. Conjunto de Datos: Iris, Algoritmo: LS relief.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 3. Conjunto de Datos: Iris, Algoritmo: ILS aleatorio.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 4. Conjunto de Datos: Iris, Algoritmo: ILS relief.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 5. Conjunto de Datos: Iris, Algoritmo: SA aleatorio.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 6. Conjunto de Datos: Iris, Algoritmo: SA relief.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 7. Conjunto de Datos: Iris, Algoritmo: SS aleatorio.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 8. Conjunto de Datos: Iris, Algoritmo: SS relief.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/experiments/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 9. Conjunto de Datos: Iris, Algoritmo: DE.\\

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 10. Conjunto de Datos: Sonar, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 11. Conjunto de Datos: Sonar, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 12. Conjunto de Datos: Sonar, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 13. Conjunto de Datos: Sonar, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 14. Conjunto de Datos: Sonar, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 15. Conjunto de Datos: Sonar, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 16. Conjunto de Datos: Sonar, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 17. Conjunto de Datos: Sonar, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/experiments/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 18. Conjunto de Datos: Sonar, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 19. Conjunto de Datos: WDBC, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 20. Conjunto de Datos: WDBC, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 21. Conjunto de Datos: WDBC, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 22. Conjunto de Datos: WDBC, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 23. Conjunto de Datos: WDBC, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 24. Conjunto de Datos: WDBC, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 25. Conjunto de Datos: WDBC, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 26. Conjunto de Datos: WDBC, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/experiments/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 27. Conjunto de Datos: WDBC, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 28. Conjunto de Datos: Spambase, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 29. Conjunto de Datos: Spambase, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 30. Conjunto de Datos: Spambase, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 31. Conjunto de Datos: Spambase, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 32. Conjunto de Datos: Spambase, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 33. Conjunto de Datos: Spambase, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 34. Conjunto de Datos: Spambase, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 35. Conjunto de Datos: Spambase, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/experiments/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 36. Conjunto de Datos: Spambase, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 37. Conjunto de Datos: Waveform, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 38. Conjunto de Datos: Waveform, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 39. Conjunto de Datos: Waveform, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 40. Conjunto de Datos: Waveform, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 41. Conjunto de Datos: Waveform, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 42. Conjunto de Datos: Waveform, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 43. Conjunto de Datos: Waveform, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 44. Conjunto de Datos: Waveform, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/experiments/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 45. Conjunto de Datos: Waveform, Algoritmo: DE.

\subsection*{Experimento 2}

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/no_weights.csv}{}{\csvlinetotablerow}%
\\
Tabla 46. Conjunto de Datos: Iris, Algoritmo: Sin pesos.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 47. Conjunto de Datos: Iris, Algoritmo: Relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 48. Conjunto de Datos: Iris, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 49. Conjunto de Datos: Iris, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 50. Conjunto de Datos: Iris, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 51. Conjunto de Datos: Iris, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 52. Conjunto de Datos: Iris, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 53. Conjunto de Datos: Iris, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 54. Conjunto de Datos: Iris, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 55. Conjunto de Datos: Iris, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/iris/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 56. Conjunto de Datos: Iris, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/no_weights.csv}{}{\csvlinetotablerow}%
\\
Tabla 57. Conjunto de Datos: Sonar, Algoritmo: Sin pesos.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 58. Conjunto de Datos: Sonar, Algoritmo: Relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 59. Conjunto de Datos: Sonar, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 60. Conjunto de Datos: Sonar, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 61. Conjunto de Datos: Sonar, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 62. Conjunto de Datos: Sonar, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 63. Conjunto de Datos: Sonar, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 64. Conjunto de Datos: Sonar, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 65. Conjunto de Datos: Sonar, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 66. Conjunto de Datos: Sonar, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/sonar/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 67. Conjunto de Datos: Sonar, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/no_weights.csv}{}{\csvlinetotablerow}%
\\
Tabla 68. Conjunto de Datos: WDBC, Algoritmo: Sin pesos.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 69. Conjunto de Datos: WDBC, Algoritmo: Relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 70. Conjunto de Datos: WDBC, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 71. Conjunto de Datos: WDBC, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 72. Conjunto de Datos: WDBC, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 73. Conjunto de Datos: WDBC, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 74. Conjunto de Datos: WDBC, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 75. Conjunto de Datos: WDBC, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 76. Conjunto de Datos: WDBC, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 77. Conjunto de Datos: WDBC, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/wdbc/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 78. Conjunto de Datos: WDBC, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/no_weights.csv}{}{\csvlinetotablerow}%
\\
Tabla 79. Conjunto de Datos: Spambase, Algoritmo: Sin pesos.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 80. Conjunto de Datos: Spambase, Algoritmo: Relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 81. Conjunto de Datos: Spambase, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 82. Conjunto de Datos: Spambase, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 83. Conjunto de Datos: Spambase, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 84. Conjunto de Datos: Spambase, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 85. Conjunto de Datos: Spambase, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 86. Conjunto de Datos: Spambase, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 87. Conjunto de Datos: Spambase, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 88. Conjunto de Datos: Spambase, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/spambase/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 89. Conjunto de Datos: Spambase, Algoritmo: DE.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/no_weights.csv}{}{\csvlinetotablerow}%
\\
Tabla 90. Conjunto de Datos: Waveform, Algoritmo: Sin pesos.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 91. Conjunto de Datos: Waveform, Algoritmo: Relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/LS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 92. Conjunto de Datos: Waveform, Algoritmo: LS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/LS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 93. Conjunto de Datos: Waveform, Algoritmo: LS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/ILS_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 94. Conjunto de Datos: Waveform, Algoritmo: ILS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/ILS_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 95. Conjunto de Datos: Waveform, Algoritmo: ILS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/SA_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 96. Conjunto de Datos: Waveform, Algoritmo: SA aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/SA_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 97. Conjunto de Datos: Waveform, Algoritmo: SA relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/Scatter_random.csv}{}{\csvlinetotablerow}%
\\
Tabla 98. Conjunto de Datos: Waveform, Algoritmo: SS aleatorio.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/Scatter_relief.csv}{}{\csvlinetotablerow}%
\\
Tabla 99. Conjunto de Datos: Waveform, Algoritmo: SS relief.

\csvreader[
  respect all,
  autotabular
  ]{statistics/waveform/DE.csv}{}{\csvlinetotablerow}%
\\
Tabla 100. Conjunto de Datos: Waveform, Algoritmo: DE.

\end{document}
